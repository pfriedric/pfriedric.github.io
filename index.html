<!DOCTYPE HTML>
<html lang="en">
<head>
 <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

 <title>Paul Friedrich</title>

 <meta name="author" content="Paul Friedrich">
 <meta name="viewport" content="width=device-width, initial-scale=1">

 <link rel="stylesheet" type="text/css" href="stylesheet.css">
 <link rel="shortcut icon" href="images/hedgehog_icon_recognizable.ico" type="image/x-icon">


 <script>
  document.addEventListener('DOMContentLoaded', (event) => {
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      document.body.classList.add('dark-mode');
    }
  });
</script>

</head>

<!-- Script to detect dark mode based on user browser preferences -->
<script type="text/javascript">
  function toggle_vis(id) {
    var e = document.getElementById(id);
    if (e.style.display == 'none')
      e.style.display = 'inline';
    else
      e.style.display = 'none';
  }
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <h1>
                	Paul Friedrich
                </h1>
              </p>
              <p> I am a Computer Science PhD student in the <a href="https://www.ifi.uzh.ch/en.html">Department of Informatics</a> at the <a href="https://www.uzh.ch/en.html">University of Zurich</a>, co-advised by <a href="https://www.ifi.uzh.ch/en/ce/people/seuken.html">Sven Seuken</a> and <a href="https://gioramponi.github.io/">Giorgia Ramponi</a>. I am associated with the <a href="https://ai.ethz.ch/">ETH AI Center</a>. In my research, I am interested in using machine and reinforcement learning to analyze and shape equilibria in games and marketplaces, and design more fair and efficient mechanisms.
              </p>
              <p> I hold a Master's (2020) and Bachelor's (2018) degree in Mathematics from <a href="https://ethz.ch/de.html">ETH Zurich</a>, advised by <a href="https://people.math.ethz.ch/~jteichma/">Josef Teichmann</a>. There, I focused on probability theory, mathematical finance, and machine learning. In 2017, I studied abroad at the Hong Kong University of Science and Technology (HKUST).
              </p>
              <p> During and after my studies, I worked as a consultant in the risk management practice of <a href="https://www.ey.com/en_ch">Ernst & Young Zurich</a>. Outside of work you can catch me running or enjoying pretty much any sport on, in and below water.
              </p>
              <p>
              	If you are offering a research internship for which I could fit well or are interested in collaborating, please reach out!
              </p>
              <p style="text-align:center">
                <a href="mailto:paul.friedrich@uzh.ch">Email</a> &nbsp/&nbsp
                <a href="data/cv_paul_friedrich.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=B-QCNA0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/pfriedric/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/paul-friedrich-924005162">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/paul.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/paul.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
              <p><span class="papertitle">Collusion of Reinforcement Learning-based Pricing Algorithms in Episodic Markets
              </span>
              <br>
              <strong>Paul Friedrich</strong>, 
              <a href="https://www.ifi.uzh.ch/en/ce/people/pasztor.html">Barna Pásztor</a>,
              <a href="https://gioramponi.github.io/">Giorgia Ramponi</a>
              <br>
              <em>Agentic Markets Workshop @ ICML</em>, 2024 (<a href="data/episodic_markets_RL_collusion_AMW-ICML.pdf" download>paper</a>)
            </p>

            <p><span class="papertitle">Scalable Mechanism Design for Multi-Agent Path Finding</span>
              <br>
              <strong>Paul Friedrich*</strong>,
              <a href="https://yulunzhang.net/">Yulun Zhang*</a>,
              <a href="https://currymj.github.io/">Michael Curry</a>,
              <a href="https://www.ludwigdierks.com/">Ludwig Dierks</a>,
              <a href="https://x.com/McaleerStephen">Stephen McAleer</a>,
              <a href="https://jiaoyangli.me/">Jiaoyang Li</a>,
              <a href="https://www.cs.cmu.edu/~sandholm/">Tuomas Sandholm</a>,
              <a href="https://www.ifi.uzh.ch/en/ce/people/seuken.html">Sven Seuken</a>
              <br>
              <em>IJCAI</em>, 2024 (<a href="https://www.ijcai.org/proceedings/2024/0007.pdf">paper</a> / <a href="https://github.com/lunjohnzhang/MAPF-Mechanism">github</a> / <a href="data/Scalable_MD_for_MAPF_IJCAI_slides.pdf">talk slides</a>)
            </p>

              <p><span class="papertitle">Market Design for Drone Traffic Management</span>
                <br>
                <a href="https://www.ifi.uzh.ch/en/ce/people/seuken.html">Sven Seuken</a>,
                <strong>Paul Friedrich</strong>,
                <a href="https://www.ludwigdierks.com/">Ludwig Dierks</a>,
                <br>
                <em>AAAI</em>, 2022 — Won 3rd best paper in its category (<a href="https://cdn.aaai.org/ojs/21493/21493-13-25506-1-2-20220628.pdf">paper</a>)
              </p>

              <p><span class="papertitle">Machine-Learning enhanced Market Design for Drone Traffic Management</span>
                <br>
                <strong>Paul Friedrich</strong>,
                <a href="https://www.ifi.uzh.ch/en/ce/people/seuken.html">Sven Seuken</a>,
                <a href="https://www.ludwigdierks.com/">Ludwig Dierks</a>
                <br>
                <em>Working paper</em>
              </p>
            </td>
          </tr>
        </tbody></table>



        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->


          <!-- </p> -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tbody><tr><td>
                <heading>News</heading>
                <ul>
                    <li>I am happy to be part of the ELLIS community.</li>
                    <li>Our paper <a href="https://arxiv.org/pdf/2306.14799.pdf">On Imitation in Mean-field Games"</a> has been accepted at NeurIPS 2023! </li>
                    <li>I was invited as lecturer at the Mediterranean Machine Learning Summer School to talk about Deep Reinforcement Learning.</li>
                    <li>I gave a talk on Reinforcement Learning and Multi-agent Learning at the New Frontiers in Learning, Control, and Dynamical Systems workshop at ICML 2023. See everyone in Hawaii!</li>
                    <li>Four papers accepted at EWRL 2023!</li>
                    <li> Designing and teaching a new course called Data Science and Machine Learning for the ETH-Ashesi Master program.</li>
                    <li>Two papers accepted at NeurIPS 2022! <a href="https://arxiv.org/abs/2207.08645">"Active Exploration for Inverse Reinforcement Learning"</a> and <a href="https://arxiv.org/abs/2210.11137">"Trust Region Policy Optimization with Optimal Transport Discrepancies: Duality and Algorithm for Continuous Actions"</a>.</li>
                    <li>Our open problem "Do you pay for Privacy in Online learning?" has been selected at COLT 2022.</li>
                    <li>Our paper "Active Exploration for Inverse Reinforcement Learning" has been accepted at the Adaptive Experimental Design and Active Learning in the Real World (ReALML) workshop at ICML 2022.</li>
                    <li>Our paper <a href="https://openreview.net/pdf?id=S3NzSD8icx9">"Learning in Markov Games: can we exploit a general-sum opponent?" </a> has been accepted as oral (~4%) at UAI 2022.</li>
                     <a href="javascript:toggle_vis('news')">show more</a>
                    <div id="news" style="display:none">
                    <li> Designing and teaching a new course at ETH Zürich with AI Center postdocs in the spring term. The course is on various advanced topics including Inverse Reinforcement Learning.</li>
                    <li>I will visit the <a href="https://simons.berkeley.edu/">Simons Institute</a> in January-February 2022 for the Learning and Games program.</li>
                    <li>Our paper "Learning in Non-Cooperative Configurable Markov Decision Process" has been accepted at NeurIPS 2021.</li>
                    <li>18.08.2021 I was selected to participate in the <a href="https://risingstars21-eecs.mit.edu/">EECS RISING STARS 2021</a>.
                    <li>In August 2021 I joined the ETH AI Center as postdoctoral researcher.</li>
                    <li>In June 2021 I completed my Ph.D.!</li>
                    <li>Our paper "Provably Efficient Learning of Transferable Rewards" has been accepted at ICML 2021.</li>
                    <li>Our paper "Dealing with multiple experts and non-stationarity in inverse reinforcement learning: an application to real-life problems" has been published in the Machine Learning journal.</li>
                    <li>Our paper ‘‘Online Learning in Non-Cooperative Configurable Markov Decision Process’’ has been accepted at RLG 2021 workshop at AAAI.</li>
                    <li>Our paper ‘‘Newton Optimization on Helmholtz Decomposition for Continuous Games’’ has been accepted at AAAI2021.</li>
                    <li>Our paper ‘‘Handling Non-Stationary Experts in Inverse Reinforcement Learning: A Water System Control Case Study’’ has been accepted at Challenges of Real World Reinforcement Learning Workshop, and ‘‘Newton-based Policy Optimization for Games’’ has been accepted at CooperativeAI workshop.</li>
                    <li>Our paper ‘‘Controlled Text Generation with Adversarial Learning’’ has been accepted at INLG 2020.</li>
                    <li>Our paper "Inverse Reinforcement Learning from a Gradient-based Learner" has been accepted at NeurIPS 2020. </li>
                    <li>Our paper ‘‘Truly Batch Model-Free Inverse Reinforcement Learning about Multiple Intentions’’ has been accepted at AISTATS 2020.</li>
                    </div>
                </ul>
            </td></tr>


            </tbody></table>





          </tbody></table> -->


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design from <a style="fonr-size:small" href="https://jonbarron.info">Jon Barron's website</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>



  </html>
